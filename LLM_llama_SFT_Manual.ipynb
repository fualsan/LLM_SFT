{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a023f0f-cf04-41b6-8ec4-7fc5efcda530",
   "metadata": {},
   "source": [
    "# Supervised Fine Tuning of llama\n",
    "\n",
    "This notebook implements a custom training loop for the llama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed2cc966-3b15-4791-8815-d5530f125020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is bf16 supported: True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "from peft import LoraConfig\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(f'Is bf16 supported: {torch.cuda.is_bf16_supported()}')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73096345-96d5-41d7-ad3c-432477032be9",
   "metadata": {},
   "source": [
    "# Model ID\n",
    "* **NOTE:** You need to have access to these models\n",
    "* Visit huggingface page and request access\n",
    "* Set your token via setting the environment variable HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311bf49e-941f-43ec-a251-aa2f4dab4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-3.2-1B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057ccbd-5dc2-4446-b58e-0e62deeecef6",
   "metadata": {},
   "source": [
    "# Quantization Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3ab73ed-60e6-4304-9fcf-b297d803701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # 4 bit quantization\n",
    "    bnb_4bit_quant_type='nf4', # normalized float 4 bit\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f9311-02d8-476d-8784-44ea001e59c1",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71619cd-f485-4c39-9027-bec3cec93ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=os.environ['HF_TOKEN'] # required for some models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836ba37-f020-4d36-a3d2-dd00454b8a1c",
   "metadata": {},
   "source": [
    "### Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efb5c414-34d3-494c-a587-f53b91bca343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed49a0-1e70-4bf3-bbe9-9a127786650f",
   "metadata": {},
   "source": [
    "### Terminators List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b72e645b-4e8b-4272-823c-880c6de31f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids('<|eot_id|>')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058246ca-70f7-4bcb-b6e3-4f57dd334a74",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "605e6ef2-5ed7-453f-b5a4-c4bb97cac0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config, \n",
    "    device_map={'':0}, \n",
    "    token=os.environ['HF_TOKEN'] # required for some models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2893d07d-efec-42c8-8144-ca6100973e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False # prevent return tuple 'past_key_values'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f99c844-e662-4ccd-be38-5adc1af5e4b0",
   "metadata": {},
   "source": [
    "### Open Ended Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "791724cd-e800-4d3b-83a3-614900ef76cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b5782-7174-4fa9-9cae-123c02044e7a",
   "metadata": {},
   "source": [
    "## Utility Function for Preprocessing\n",
    "\n",
    "* llama 3 special tokens: https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "* llama 3.1 special tokens: https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5afa8316-9df2-4edc-921f-f6e71c08ecfb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# META LLAMA 3.1 FULL MESSAGE\n",
    "\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful AI assistant for travel tips and recommendations<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What is France's capital?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "Bonjour! The capital of France is Paris!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What can I do there?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "Paris, the City of Light, offers a romantic getaway with must-see attractions like the Eiffel Tower and Louvre Museum, romantic experiences like river cruises and charming neighborhoods, and delicious food and drink options, with helpful tips for making the most of your trip.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Give me a detailed list of the attractions I should visit, and time it takes in each one, to plan my trip accordingly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "# SFT EXAMPLE\n",
    "\n",
    "text = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: {datetime.now().strftime('%d %B %Y')}\n",
    "\n",
    "You are a helpful assistant.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{instruction} <|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "{output} <|eot_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "022edf8a-536c-4c75-a95c-33f7c0a63fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: <|begin_of_text|> is added by tokenizer ???\n",
    "\n",
    "training_prompt = lambda user_instruction, assistant_response: \\\n",
    "f\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{user_instruction} <|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|> \n",
    "{assistant_response} <|eot_id|>\"\"\"\n",
    "\n",
    "\n",
    "generation_prompt = lambda user_instruction: \\\n",
    "f\"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{user_instruction} <|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f8acbd-3521-4eb2-b668-e6ed0d0f4d23",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82646bc9-351c-41fa-a3d8-93c0a485c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.amp.autocast('cuda')\n",
    "def generate(\n",
    "    prompt, \n",
    "    eos_token_id=terminators,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.90,\n",
    "    top_p=0.9,\n",
    "    num_beams=1,\n",
    "    num_return_sequences=1,\n",
    "    skip_special_tokens=True\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    input_text = generation_prompt(prompt)\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors='pt').to(device)\n",
    "    input_token_len = len(inputs['input_ids'][0])\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs, # input token ids and attention mask (as kwargs)\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=eos_token_id,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        num_beams=num_beams,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=False,\n",
    "    )\n",
    "\n",
    "    #print(f'LEN: {len(outputs)}')\n",
    "    #print('KEYS', outputs.keys())\n",
    "    #print(f'Scores: {outputs[\"sequences_scores\"]}')\n",
    "    \n",
    "    for _gen_output in outputs['sequences']:\n",
    "        _generated_response = tokenizer.decode(_gen_output[input_token_len:], skip_special_tokens=skip_special_tokens)\n",
    "                \n",
    "        print(_generated_response)\n",
    "        print('#'*35)\n",
    "        print()\n",
    "\n",
    "\n",
    "@torch.amp.autocast('cuda')\n",
    "def return_model_output(prompt, cut=True):\n",
    "    model.eval()\n",
    "    \n",
    "    text = generation_prompt(prompt)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "    input_token_len = len(inputs['input_ids'][0])\n",
    "    \n",
    "    return model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e749408e-82a4-493c-93e3-343f2577fd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits'])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = return_model_output('hello world')\n",
    "    print(pred.keys())\n",
    "    logits = pred['logits']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446190a-5352-4d08-8da5-f6f123ad70fe",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00dd1539-28b6-481f-88bc-93582c6ee9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.amp.autocast('cuda')\n",
    "def train(model, optimizer, num_epochs, user_instruction, assistant_response):\n",
    "    full_prompt = training_prompt(user_instruction, assistant_response)\n",
    "\n",
    "    inputs = tokenizer(full_prompt, return_tensors='pt').to(device)    \n",
    "    # make shifted\n",
    "    input_ids = inputs['input_ids'][:, :-1]\n",
    "    attention_mask = inputs['attention_mask'][:, :-1]\n",
    "    target_ids = inputs['input_ids'][:, 1:]\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_ids = model(input_ids=input_ids, attention_mask=attention_mask)['logits']\n",
    "        \n",
    "        loss = F.cross_entropy(\n",
    "            pred_ids.view(-1, pred_ids.shape[-1]),\n",
    "            target_ids.view(-1),\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1849b18d-5573-48c6-8fd5-660f4ef8395d",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97fcadda-647f-46b3-b820-614a7f767720",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = bnb.optim.Adam8bit(\n",
    "    model.parameters(), \n",
    "    lr=1e-4, \n",
    "    is_paged=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4ea3a-ba1b-4a10-abae-1f72006e6024",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10621c96-5144-4cba-93d0-6cb45841749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_INSTRUCTION = ['Türkiyenin en güzel şehiri neresidir?']\n",
    "ASSISTANT_RESPONSE = [\"Türkiyenin şehiri Çanakkale'dir.\"]\n",
    "\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12dce5-13af-40fa-8ae5-f89e92c1ec9a",
   "metadata": {},
   "source": [
    "#### Generation Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4e2706b-eba3-43e1-a419-a74ce284fe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Türkiyenin en güzel şehiri neresidir?\n",
      "\n",
      "\n",
      "Bursa!\n",
      "\n",
      "Bursa, Türkiye'nin en güzeli ve en beautiful şehriです。 Bursa, İstanbul ile bir ikiye yakınıにある, Balkan Yaradası'nın kuzeyinde yer alan ve iklimi tropik olarak bir birीकlidir. Bursa, Istanbul'ın en beautiful ve en küçük şehriye de cheerslidir.\n",
      "###################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _instruction in USER_INSTRUCTION:\n",
    "    print(f'Instruction: {_instruction}')\n",
    "    generate(_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83c326da-727a-4949-9619-099c8e66d406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 7.2135\n",
      "Epoch: 2, Loss: 6.3118\n",
      "Epoch: 3, Loss: 5.5616\n",
      "Epoch: 4, Loss: 4.9536\n",
      "Epoch: 5, Loss: 4.4676\n",
      "Epoch: 6, Loss: 4.0281\n",
      "Epoch: 7, Loss: 3.6435\n",
      "Epoch: 8, Loss: 3.2946\n",
      "Epoch: 9, Loss: 2.9835\n",
      "Epoch: 10, Loss: 2.6946\n",
      "Epoch: 11, Loss: 2.4259\n",
      "Epoch: 12, Loss: 2.1721\n",
      "Epoch: 13, Loss: 1.9278\n",
      "Epoch: 14, Loss: 1.7092\n",
      "Epoch: 15, Loss: 1.5211\n",
      "Epoch: 16, Loss: 1.3678\n",
      "Epoch: 17, Loss: 1.2323\n",
      "Epoch: 18, Loss: 1.1024\n",
      "Epoch: 19, Loss: 0.9749\n",
      "Epoch: 20, Loss: 0.8584\n"
     ]
    }
   ],
   "source": [
    "train(model, optim, NUM_EPOCHS, USER_INSTRUCTION, ASSISTANT_RESPONSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74931a5-8713-434e-991d-f6ebf93ed4fd",
   "metadata": {},
   "source": [
    "#### Generation After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed83350b-7ab4-4a75-8d2e-5f769114ca6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Türkiyenin en güzel şehiri neresidir?\n",
      " \n",
      "Türkiyenin en güzel şehiri Çanakkale'dir  2011 year'de 3.7 milyonlúkurlu Çanakkale'dir  2018 year'de 4.2 milyonlúkurlu  Çanakkale'dir  2019 year'de 3.5 milyonlúkurlu  Çanakkale'dir  2018 year'de 4.2 milyonlúkurlu  Çanakkale'dir  2019 year'de 3\n",
      "###################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _instruction in USER_INSTRUCTION:\n",
    "    print(f'Instruction: {_instruction}')\n",
    "    generate(_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8859e4a8-66a9-49a7-85cc-19d6cf61d3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:junk]",
   "language": "python",
   "name": "conda-env-junk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
